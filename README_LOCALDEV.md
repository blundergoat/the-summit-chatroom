# Local Development Guide

Two ways to run The Summit locally: **Docker Compose** (everything containerised) or **bare-metal** (PHP and Python running directly on your machine). Both need an LLM — either Ollama running locally or AWS Bedrock in the cloud.

## Quick Start

### Option A: Docker Compose (recommended for first run)

Everything runs in containers. No local PHP/Python install needed.

```bash
cp .env.example .env
docker compose up --build
# Open http://localhost:8082
```

First run pulls the LLM model (~9GB for qwen2.5:14b) — this takes a few minutes.

### Option B: Bare-metal (recommended for development)

Runs PHP and Python directly. Faster iteration, no container rebuilds.

```bash
./scripts/setup-initial.sh    # Install all dependencies
./scripts/start-dev.sh        # Start PHP + Python + Ollama
# Open http://localhost:8082
```

## Prerequisites

### For Docker Compose

- Docker and Docker Compose
- ~12GB free disk space (LLM models)

### For bare-metal

- PHP 8.2+
- Composer
- Python 3.12+
- pip3
- Ollama installed locally (https://ollama.com)
- The `strands-php-client` repo cloned as a sibling directory:

```
projects/
├── the-summit-chatroom/         # This repo
└── strands-php-client/        # Required — local Composer path dependency
```

## Architecture

Both modes run the same three services:

```
Browser ──► PHP Symfony app ──► Python FastAPI agent ──► LLM (Ollama or Bedrock)
               (port 8082)         (port 8081)           (port 11434)
```

### Docker Compose mode

5 containers with automatic dependency ordering:

| Service | Image | Port | Purpose |
|---------|-------|------|---------|
| **ollama** | ollama/ollama | 11434 | Local LLM server |
| **ollama-pull** | ollama/ollama | — | One-shot model download, then exits |
| **agent** | Built from `strands_agents/Dockerfile` | 8081 → 8000 | Python FastAPI agent (Strands SDK) |
| **mercure** | dunglas/mercure | 3100 | Real-time SSE hub for streaming mode |
| **app** | Built from `Dockerfile` | 8082 → 8080 | PHP Symfony chat UI |

Startup order: ollama → ollama-pull → agent → mercure → app

Containers talk via Docker networking (e.g. `http://agent:8000`, `http://ollama:11434`).

### Bare-metal mode

3 processes managed by `start-dev.sh`:

| Process | Port | What runs |
|---------|------|-----------|
| **Ollama** | 11434 | `ollama serve` (started automatically if not running) |
| **Python agent** | 8081 | `uvicorn api.server:app` via the project venv |
| **PHP app** | 8082 | `php -S 0.0.0.0:8082 -t public` |

Processes talk via `localhost`. Mercure is not started — streaming mode is disabled, sync mode works fully. Use Docker Compose if you need streaming.

## Environment Configuration

All config lives in `.env`. Docker Compose and bare-metal both read from it.

```bash
cp .env.example .env
```

### Choosing a model provider

#### Ollama (default — local, free)

No credentials needed. The model runs on your machine.

```env
MODEL_PROVIDER=ollama
OLLAMA_MODEL=qwen2.5:14b
```

#### AWS Bedrock (cloud)

Uses cloud-hosted models. Requires AWS credentials.

```env
MODEL_PROVIDER=bedrock
AWS_ACCESS_KEY_ID=your-access-key-id
AWS_SECRET_ACCESS_KEY=your-secret-access-key
AWS_SESSION_TOKEN=              # Only if using temporary credentials
AWS_DEFAULT_REGION=ap-southeast-2
MODEL_ID=us.anthropic.claude-sonnet-4-20250514-v1:0
```

### Changing the Ollama model

Edit `.env`:

```env
OLLAMA_MODEL=mistral
```

Good options: `qwen2.5:14b` (default, 9GB), `mistral` (4GB), `llama3.1` (4.7GB), `gemma2` (5.4GB).

Smaller models are faster but produce lower quality council debates. The 14b parameter model is a good balance for machines with 16GB+ RAM.

**For Docker Compose**: restart to pull the new model:

```bash
docker compose down
docker compose up
```

**For bare-metal**: the start script pulls automatically:

```bash
# Either set in .env and restart, or override inline:
OLLAMA_MODEL=mistral ./scripts/start-dev.sh
```

**To pull a model manually**:

```bash
ollama pull mistral
```

### Hardware requirements for Ollama

| Model | Size | RAM needed | GPU VRAM | Speed |
|-------|------|-----------|----------|-------|
| mistral (7B) | ~4GB | 8GB | 8GB | Fast |
| llama3.1 (8B) | ~4.7GB | 8GB | 8GB | Fast |
| qwen2.5:14b | ~9GB | 16GB | 16GB | Moderate |
| llama3.1:70b | ~40GB | 64GB | 48GB | Slow |

CPU inference works but is significantly slower. A GPU with sufficient VRAM is recommended.

### Other environment variables

These are set automatically by `start-dev.sh` and `docker-compose.yml`. You typically don't need to change them:

| Variable | Docker value | Bare-metal value | Purpose |
|----------|-------------|-----------------|---------|
| `AGENT_ENDPOINT` | `http://agent:8000` | `http://localhost:8081` | PHP → Python agent URL |
| `OLLAMA_HOST` | `http://ollama:11434` | `http://localhost:11434` | Python agent → Ollama URL |
| `MERCURE_URL` | `http://mercure:3100/...` | *(empty)* | PHP → Mercure publish URL |
| `MERCURE_PUBLIC_URL` | `http://localhost:3100/...` | *(empty)* | Browser → Mercure subscribe URL |
| `MERCURE_JWT_SECRET` | `the-summit-mercure-secret` | *(empty)* | JWT signing for Mercure |
| `APP_SECRET` | `the-summit-dev-secret-change-me` | same | Symfony CSRF/session secret |

## Scripts Reference

All scripts are in the `scripts/` directory.

### Setup

| Script | Purpose |
|--------|---------|
| `setup-initial.sh` | First-time setup: checks prerequisites, copies `.env`, runs `composer install`, creates Python venv, installs pip dependencies |
| `setup-verify.sh` | Verifies the environment: system tools, project files, PHP/Python dependencies, dev tools, runs PHPUnit + PHPStan + CS check |

### Running

| Script | Purpose |
|--------|---------|
| `start-dev.sh` | Starts Ollama + Python agent + PHP app. Press Ctrl+C to stop all. |
| `health-check-localdev.sh` | Checks if all services are running and responsive (Ollama API, model loaded, agent endpoints, PHP app, Mercure, port usage) |

### Dependencies

| Script | Purpose |
|--------|---------|
| `dependencies-install.sh [--php] [--python]` | Install from lock files (exact versions) |
| `dependencies-update.sh [--php] [--python]` | Update to latest versions within constraints, runs security audit and smoke tests |

### Quality

| Script | Purpose |
|--------|---------|
| `preflight-checks.sh` | All 12 quality gates: composer validate, security audit, code style, complexity, PHPMD, PHPStan, Twig lint, Python syntax, Docker config, tests, coverage, mutation testing |
| `preflight-checks.sh --mutate` | Include Infection mutation testing (adds ~2-5s) |
| `preflight-checks.sh --coverage-min=90` | Override minimum coverage threshold (default 80%) |

## Sync vs Streaming Mode

The app supports two modes for receiving agent responses:

### Sync mode (both Docker and bare-metal)

The browser sends a request to `/chat` and waits for all three agents to respond sequentially. Takes ~30-45 seconds depending on the model and hardware. Simple and reliable.

### Streaming mode (Docker Compose only)

Requires Mercure. The browser gets an immediate response with a Mercure topic, subscribes via EventSource (SSE), and receives tokens in real-time as each agent generates them. Word-by-word output like ChatGPT.

Streaming is automatically enabled when Mercure is available (Docker Compose) and disabled when it's not (bare-metal).

## Troubleshooting

### "strands-php-client not found"

The PHP app depends on the `strands-php-client` package via a Composer path repository. Clone it as a sibling directory:

```bash
cd ..
git clone https://github.com/blundergoat/strands-php-client.git
cd the-summit-chatroom
composer install
```

### "Address already in use" on start

Another process is using port 8081 or 8082. Check what's running:

```bash
./scripts/health-check-localdev.sh    # Shows what's listening on each port
```

Or override the ports:

```bash
AGENT_PORT=9081 APP_PORT=9082 ./scripts/start-dev.sh
```

### Ollama model is slow

- Check if you have GPU acceleration: `ollama ps` shows if the model is using GPU
- Try a smaller model: `OLLAMA_MODEL=mistral ./scripts/start-dev.sh`
- CPU-only inference for 14B models takes 30-60 seconds per agent response

### Docker build fails at "strands-php-client"

The `docker-compose.yml` uses `additional_contexts` to access the sibling directory. Make sure the directory exists:

```bash
ls ../strands-php-client/composer.json    # Should exist
docker compose up --build
```

### Agent returns errors about model not found

The Ollama model hasn't been pulled yet:

```bash
# Check what models are available
ollama list

# Pull the configured model
ollama pull qwen2.5:14b
```

### Python agent won't start (bare-metal)

Check the venv exists and has dependencies:

```bash
./scripts/setup-verify.sh    # Checks everything
# Or manually:
strands_agents/.venv/bin/python -c "import strands; import fastapi"
```
